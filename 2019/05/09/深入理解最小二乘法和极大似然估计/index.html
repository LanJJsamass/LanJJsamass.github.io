<!DOCTYPE html>
<html lang="zh-Hans">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="1、由来参数极大似然估计和最小二乘法全部都是高斯发明的，但是前者多将其归功于费希尔。这两者全部都是用来估计参数的一种方法，先看看这两者的定义：1）极大似然估计：一般来说，设总体X为离散型随机变量，分布中含有未知参数θ（可以是向量），其样本（X1,X2,X3,…Xn）为n维离散型随机变量，对于一个抽样">
    

    <!--Author-->
    
        <meta name="author" content="YWP">
    

    <!-- Title -->
    
    <title>Hexo</title>

    <!-- Bootstrap Core CSS -->
    <link href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Noto+Serif:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

    <!-- Content -->
    <section class="article-container">
<!-- Back Home -->
<a class="nav-back" href="/">
    <i class="fa fa-puzzle-piece"></i>
</a>

<!-- Page Header -->
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Halo</h1>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Main Content -->
            <div class="post-content col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h1 id="1、由来"><a href="#1、由来" class="headerlink" title="1、由来"></a>1、由来</h1><p>参数极大似然估计和最小二乘法全部都是高斯发明的，但是前者多将其归功于费希尔。这两者全部都是用来估计参数的一种方法，先看看这两者的定义：<br>1）极大似然估计：一般来说，设总体X为离散型随机变量，分布中含有未知参数θ（可以是向量），其样本（X1,X2,X3,…Xn）为n维离散型随机变量，对于一个抽样，得到样本观测值(x1,x2,x3,…xn)，称：<br>L(x1,x2,x3,x4,…xn;θ)=P(X1=x1,X2=x2,…Xn=xn)为似然函数，称：<br>l(x1,x2,x3,x4,…xn;θ)=lnL(x1,x2,x3,x4,…xn;θ)为对数似然函数，称满足<br>L(x1,x2,x3,x4,…xn;θ^)=maxL(x1,x2,x3,x4,…xn;θ)的θ^为θ的极大似然估计。<br>2）最小二乘法：（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。<br>只是从定义上看，最小二乘法相较于极大似然估计太简单了，上面这个极大似然估计的定义太抽象了，本质上来理解极大似然估计就是，你已经知道了一组数据，这组数据在你看来就是世界本来的模样。但是，仅仅这么简单的理解还是不行的，你需要从概率上来进行解释。极大似然估计就是你相信你看到的这些数据就是这个事件最想让你看到的结果，你现在通过将他们的概率连乘并且取合适的参数将这个概率达到最大。最小二乘法就有点简单粗暴了，就是将你的预测值和实际值的差值求和，取合适的参数将这个求和值达到最小的方法。  </p>
<h1 id="2、实际应用"><a href="#2、实际应用" class="headerlink" title="2、实际应用"></a>2、实际应用</h1><p>我们最开始接触最小二乘法是在线性回归上面，我们当时去求预测值和实际值差值的平方最小。这里为什么会用平方，而不用绝对值和四次方，在后面讲到它的概率解释的时候会交代清楚。  </p>
<h2 id="1）极大似然估计："><a href="#1）极大似然估计：" class="headerlink" title="1）极大似然估计："></a>1）极大似然估计：</h2><p>在线性回归分析中，我们可以通过极大似然估计进行参数的选择。首先，我们假设不同x的误差值互相独立且同分布，这样子，我们的差值就满足高斯分布的基本特征。所以，我们在这里假设的是我们的误差值满足正态分布。因此，我们误差值的概率也会满足正态分布的概率密度函数，即很常见的正态分布函数。在这里要重点注意，我们在求极大似然估计就是将各个误差值的概率连乘，取概率最大即可。这里会有疑问，为什么我的误差值概率连乘取最大值，就会使得在这个参数下最适合我们这个模型？这里其实是之前我没有掌握好极大似然估计的意义。极大似然估计的核心思想更有点哲理在里面。就是，极大似然估计就是你非常相信你看到的这些数据已经是模型最想让你看到的这些。可以这样子来想象，在线性回归模型中，你一定知道会有一条直线满足你的回归模型，但是，你求出来的模型和实际值总是存在一个差值，这个差值可以想象成是一个神秘力量导致的，而你有十分相信自己就是真命天子，你看到的这个差值就是最适合这条神秘直线的差值，你现在就是拼命让发生这个差值的概率达到最大。所以你要通过极大似然估计求参数，将这个概率达到最大。这就是极大似然估计。  </p>
<p>##2）最小二乘法：<br>这里为什么要使用平方而不是绝对值和四次方，我们可以从上面求极大似然估计的过程中理解，在求极大似然估计的过程中，我们使用的就是平方，因此，从概率上解释就是，当误差值满足正态分布的时候，我们的极大似然估计和最小二乘法其实是一个东西。</p>
<h1 id="3、相通点"><a href="#3、相通点" class="headerlink" title="3、相通点"></a>3、相通点</h1><p>当误差值满足正态分布，那么极大似然估计和最小二乘法是一个东西。当误差值不满足正态分布的时候，就不是一个东西。</p>
<h1 id="4、不同点"><a href="#4、不同点" class="headerlink" title="4、不同点"></a>4、不同点</h1><p>最小二乘法其实更加满足人类认识的本质，本质就是一种权衡的思想。更加简单和粗暴。将预测值和实际值的差值平方求和，通过调整参数使这个和达到最小就是最理想的理论模型。而极大似然估计并不是这样子的，就是首先我相信已经有一条线能够使我这个模型很好的契合，我现在通过将我这个差值发生的概率最大来使我们这个模型成立。前者更加接近人类思考的本源，后者是从哲理方面出发，从概率论方面解释的。</p>

 
                <!-- Meta -->
                <div class="post-meta">
                    <hr>
                    <br>
                    <div class="post-tags">
                        
                    </div>
                    <div class="post-date">
                        2019 年 05 月 09 日
                    </div>
                </div>
            </div>

            <!-- Comments -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- Disqus Comments -->


            </div>
        </div>
    </div>
</article>
</section>

    <!-- Scripts -->
    <!-- jQuery -->
<script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<script type="text/javascript">
	console.log('Hexo-theme-hollow designed by zchen9 🙋 © 2015-' + (new Date()).getFullYear());
</script>

    <!-- Google Analytics -->
    

</body>

</html>